{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\gwher\\AppData\\Local\\Temp\\ipykernel_40156\\1630407979.py\", line 5, in <module>\n",
      "    from utils_reversal import choose, rand_matching, kagebunshin, random_pairing\n",
      "  File \"c:\\Users\\gwher\\OneDrive\\Desktop\\llm-jepa\\utils_reversal.py\", line 6, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\site-packages\\torch\\__init__.py\", line 2120, in <module>\n",
      "    from torch._higher_order_ops import cond\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\site-packages\\torch\\_higher_order_ops\\__init__.py\", line 1, in <module>\n",
      "    from .cond import cond\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\site-packages\\torch\\_higher_order_ops\\cond.py\", line 5, in <module>\n",
      "    import torch._subclasses.functional_tensor\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 42, in <module>\n",
      "    class FunctionalTensor(torch.Tensor):\n",
      "  File \"c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 258, in FunctionalTensor\n",
      "    cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "c:\\Users\\gwher\\anaconda3\\envs\\reversal\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:258: UserWarning: Failed to initialize NumPy: \n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      " (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from utils_reversal import choose, rand_matching, kagebunshin, random_pairing\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "num_GA_entities = 50000           # number of group A entities\n",
    "\n",
    "# multiplicity\n",
    "# comb = 1     # abstract concept level\n",
    "comb = 10      # attaching surface-form names\n",
    "\n",
    "num_GB_entities = int(num_GA_entities * 0.6)\n",
    "GA_entities = [\"<s_{}>\".format(i) for i in range(num_GA_entities)]\n",
    "GB_entities = [\"<t_{}>\".format(i) for i in range(num_GB_entities)]\n",
    "all_entities = GA_entities + GB_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_token_dict(l, num_token=1, comb=None):\n",
    "    \"\"\"\n",
    "    l: a list of distinct entity names\n",
    "    num_token: number of tokens for each entity\n",
    "    comb: multiplicity of f/l names (if not None)\n",
    "\n",
    "    returns: a dict mapping each entitiy in l to its token list\n",
    "    \"\"\"\n",
    "    if num_token == 1:\n",
    "        # identity map\n",
    "        assert comb is None\n",
    "        return {e: [e] for e in l}\n",
    "    \n",
    "    assert not (comb is None)\n",
    "    # add multiplicity for first two tokens, then fill the tails with unique tokens\n",
    "    assert num_token >= 2\n",
    "    num_tail_tokens = num_token - 2\n",
    "    assert type(comb) == int and len(l) % comb == 0\n",
    "    token_dict = dict()\n",
    "    \n",
    "    first, last = [\"<f_{}>\".format(i) for i in range(len(l)//comb)], [\"<l_{}>\".format(i) for i in range(len(l)//comb)]\n",
    "    first, last = kagebunshin(first, comb), kagebunshin(last, comb)\n",
    "    if comb == 1:\n",
    "        tokens = [(first[jj], last[jj]) for jj in range(len(first))]\n",
    "    else:\n",
    "        tokens = rand_matching(first, last)\n",
    "    assert len(tokens) == len(l)\n",
    "\n",
    "    for j in range(len(l)):\n",
    "        e = l[j]\n",
    "        # sanity check\n",
    "        assert e.count(\"<\") == e.count(\">\") == 1\n",
    "        assert e[0] == \"<\" and e[-1] == \">\"\n",
    "        tail = [\"<{}_{}>\".format(e.strip(\"><\"), i) for i in range(num_tail_tokens)]\n",
    "        token_dict[e] = list(tokens[j]) + tail\n",
    "    return token_dict\n",
    "    \n",
    "def form_items(arr, ty, entity_token_dict, vocab):\n",
    "    h,r,t = arr\n",
    "\n",
    "    item_list = []\n",
    "    input_text = [[vocab[tok] for tok in entity_token_dict[h]], [vocab[r]], [vocab[\"<mask>\"]]]\n",
    "    target_text = [[vocab[tok] for tok in entity_token_dict[t]]]\n",
    "    \n",
    "    item_list.append({\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text,\n",
    "        \"type\": ty,\n",
    "    })\n",
    "    return item_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80000/80000 [02:06<00:00, 634.12it/s] \n"
     ]
    }
   ],
   "source": [
    "if comb is None:\n",
    "    ent_token_dict = build_token_dict(all_entities)\n",
    "else:\n",
    "    assert type(comb) == int\n",
    "    ent_token_dict = build_token_dict(all_entities, num_token=2, comb=comb)\n",
    "\n",
    "ent_token_set = set()\n",
    "for v in ent_token_dict.values():\n",
    "    ent_token_set |= set(v)\n",
    "\n",
    "vocab = dict()\n",
    "for tok in list(ent_token_set) + [\"<mask>\"]:\n",
    "    assert tok not in vocab\n",
    "    vocab[tok] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:03<00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/atomic/test: 300000 90000 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_rel_pairs = 6\n",
    "train, atomic, test = [], [], []\n",
    "\n",
    "for i in tqdm(range(num_rel_pairs)):\n",
    "    rel, rel_inv = \"<r_{}>\".format(i), \"<r_{}_inv>\".format(i)     # add _inv if want r != r^{-1}\n",
    "    for tok in [rel, rel_inv]:\n",
    "        assert tok not in vocab\n",
    "        vocab[tok] = len(vocab)\n",
    "\n",
    "    for (ent1, ent2) in random_pairing(GA_entities):\n",
    "        # add both directions into training set for learning the rules\n",
    "        train += form_items([ent1, rel, ent2], 'train', entity_token_dict=ent_token_dict, vocab=vocab)\n",
    "        train += form_items([ent2, rel_inv, ent1], 'train', entity_token_dict=ent_token_dict, vocab=vocab)\n",
    "\n",
    "    for (ent1, ent2) in random_pairing(GB_entities):\n",
    "        # add one direction into training set, and the other into test set\n",
    "        if random.uniform(0,1) <= 0.5:\n",
    "            atomic += form_items([ent1, rel, ent2], 'atomic', entity_token_dict=ent_token_dict, vocab=vocab)\n",
    "            test += form_items([ent2, rel_inv, ent1], 'test', entity_token_dict=ent_token_dict, vocab=vocab)\n",
    "        else:\n",
    "            atomic += form_items([ent2, rel_inv, ent1], 'atomic', entity_token_dict=ent_token_dict, vocab=vocab)\n",
    "            test += form_items([ent1, rel, ent2], 'test', entity_token_dict=ent_token_dict, vocab=vocab)\n",
    "\n",
    "print(\"train/atomic/test:\", len(train), len(atomic), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16015\n"
     ]
    }
   ],
   "source": [
    "# pad token at the end\n",
    "for tok in [\"<pad>\", \"<PRED>\"]:\n",
    "    assert tok not in vocab\n",
    "    vocab[tok] = len(vocab)\n",
    "\n",
    "assert len(vocab) == len(set(vocab.keys())) == len(set(vocab.values()))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 3000\n",
    "\n",
    "if comb is None:\n",
    "    dataset = \"inversionid.{}.{}\".format(num_GA_entities, num_GB_entities)\n",
    "else:\n",
    "    dataset = \"inversionidcomb{}.{}.{}\".format(comb, num_GA_entities, num_GB_entities)\n",
    "\n",
    "os.makedirs(\"data/{}\".format(dataset), exist_ok=True)\n",
    "\n",
    "with open(\"data/{}/train.json\".format(dataset), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(train + atomic, f)\n",
    "\n",
    "probes = {\n",
    "    \"train\": choose(train, test_size),\n",
    "    \"atomic\": choose(atomic, test_size),\n",
    "    \"test\": choose(test, test_size),\n",
    "}\n",
    "\n",
    "with open(\"data/{}/valid.json\".format(dataset), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(probes, f)\n",
    "with open(\"data/{}/vocab.json\".format(dataset), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(vocab, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reversal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
